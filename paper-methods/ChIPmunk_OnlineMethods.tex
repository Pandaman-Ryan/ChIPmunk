\documentclass[12pt]{article}

% Imports
\usepackage{hyperref}
\usepackage[margin=0.5in]{geometry}
\usepackage{ctable}
\usepackage{array}
\usepackage{titlesec}
\usepackage{amsmath}

% Paragraph spacing
\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}

% Default font
\renewcommand*{\familydefault}{\sfdefault}

% title spacing
\titlespacing*{\section}
{0pt}{2pt}{0pt}
\titlespacing*{\subsection}
{0pt}{2pt}{0pt}

% table lines
\newcolumntype{?}{!{\vrule width 1pt}}

% hyperlinks
\hypersetup{
  breaklinks=true,  % so long urls are correctly broken across lines
  colorlinks=true,
  urlcolor=blue,
  linkcolor=red,
  citecolor=red,
 }

\begin{document}

\section*{Online Methods}

\subsection*{ChIPmunk model}

ChIPmunk models the shearing, pulldown, PCR, and sequencing steps of ChIP-seq and learns the parameters for each of these steps described in Table 1. 

\subsubsection*{Shearing}

Shearing is the process of randomly splitting DNA into smaller pieces called fragments. This is needed to sequence the DNA, otherwise Next-Gen Sequencers are unable to make accurate reads. There are currently two different methodologies to generating these types of reads one of which you can directly ascertain the fragment lengths, paired end, and the other you cannot, single end.

Paired end

Paired end sequencing is the process of generating reads by sequencing both ends of a sheared fragment which allows the full length of the fragment sequenced to be known from the BAM file alone. By best fitting a set of these fragments, we learned that their lengths closely follow the gamma distribution. Using this concept, we can take paired end ChIP sequencing data and read in fragment lengths at random points in the genome in order to find the parameters of a gamma distribution that best models these lengths. In particular, the learn module reads in 10,000 randomly selected fragment lengths from the inputted paired end BAM file. These are filtered to remove error fragments that exceed the length $3 * \text{median}$. Using the fragment lengths can generate their mean, $\mu = \dfrac{\sum_{i=1}^{n}X_i}{n}$. And we can generate the parameters of the gamma distribution using the method of moments with the shape parameter, $k$, and scale parameter, $\theta$: 
$$k\theta = \mu$$
$$k{\theta}^2 = \dfrac{1}{n}\sum_{i=1}^n(X_i - \mu)^2$$  
we can solve for $k$ and $\theta$ as $k = \dfrac{\mu}{\theta}$ and $\theta = \dfrac{1}{n\mu}\sum_{i=1}^{n}(X_i - \mu)^2$
Using these new found parameters we can generate fragments of appropriate sizes to be used in the pulldown phase of this simulator.

Single end

To estimate the fragment length distribution from single-end reads, we assumed the length distribution followed gamma distribution with the mean value as $\mu$ and variance as $v$, and used reads located inside ChIP-seq peaks to estimate $\mu$ and $v$.

We processed a ChIP-seq peak at a time. For each read in a $peak_i$, we added its leftmost position into a list $\{start\}_{peak_i}$ if the read is on the forward strand, or its rightmost position into a list $\{end\}_{peak_i}$ if the read is on the reverse strand. The center point of this peak was calculated by equation (1)

$$center_{peak_i} = \frac{mean(\{start\}_{peak_i}) + mean(\{end\}_{peak_i})}{2}\;\;\;(1)$$

As is shown in equation (2) and (3), for every $peak_i$ we offset the coordinates in $\{start\}_{peak_i}$ and $\{end\}_{peak_i}$ by its center point, so that the coordinates of start points and end points were normalized and symmetric to zero. Then we concatenated the lists of all ChIP-seq peaks together to form $\{start\}$ and $\{end\}$. The mean value of fragment length mu can be estimated by formulation (4).

$$\{start\} = \oplus_{i=0}^{n} (\{start\}_{peak_i} - center_{peak_i})\;\;\;(2)$$
$$\{end\} = \oplus_{i=0}^{n} (\{end\}_{peak_i} - center_{peak_i})\;\;\;(3)$$
$$\mu = mean(\{end\}) - mean(\{start\})\;\;\;(4)$$

For both \{start\} and \{end\}, we calculated their probability density functions, cumulative density functions and expected density functions. Here, the expected density function $EDF(x)$ is defined as the expected deviation of a random element in the list to x, as is shown in (5) and (6) in which S is a random element in \{start\} and E is a random element in \{end\}
$$EDF_{start}(x) = E(|S - x|)\;\;\;(5)$$
$$EDF_{end}(x) = E(|E - x|)\;\;\;(6)$$

Since we have calculated $\mu$, we can reduce the density function of the fragment length distribution into $p_v(x)$. We constructed a score function for $v$ shown as equation (7)- (11). Intuitively, if we can have a correct guess of $v$, $F(v)$ should equals to zero. Thus, we conducted a binary search for $v$ between 1000 and 10000, and searched for the best $v$ that can minimize $|F(v)|$.

$$F(v) = E_v(|S + \frac{L}{2}|) + E_v(|E - \frac{L}{2}|) - E(|S + \frac{\mu}{2}|) + E(|E- \frac{\mu}{2}|)\;\;\;(7)$$
$$E_v(|S + L/2|) = \sum_{x=0}^\infty p_v(x) * EDF_{start}(-\frac{x}{2})\;\;\;(8)$$
$$E_v(|E - L/2|) = \sum_{x=0}^\infty p_v(x) * EDF_{end}(\frac{x}{2})\;\;\;(9)$$
$$E(|S + \frac{\mu}{2}|)=EDF_{start}(x)\;\;\;(10)$$
$$E(|E - \frac{\mu}{2}|)=EDF_{end}(x)\;\;\;(11)$$

In practice, we slightly offlist the last two items in (7) and changed it into (12), which provided us more accurate estimation for the variance of the fragment length distribution.
$$F(v) = E_v(|S + \frac{L}{2}|) + E_v(|E - \frac{L}{2}|) - E(|S + \frac{\mu}{2} - \frac{E- \frac{\mu}{2}}{4}|) - E(|E- \frac{\mu}{2} - \frac{S + \frac{\mu}{2}}{4}|)\;\;\;(12)$$


\subsubsection*{Pulldown}

The pulldown step of the simulator represents purifying the precipitated DNA by washing away the sheared fragments that were not bound by the target protein or histone modification. These filtered fragments are then marked to be sequenced. The filtering process is not perfect and therefore some fragments not bound will also be pulled down to be sequenced which we label as noise. In order to model this process we start at a given position in the genome based on the region specified by the user. The random fragment is generated from the gamma distribution learned through the shearing process and based on the fragment's location we can generate its peak score. The peak score is a metric based on the peak intensity read from the bed file that the user has inputted. The higher the intensity of the peak the higher the probability that a fragment will be mapped to that location. However, this isn't the only factor to determine if the fragment will be bound. Let $P_b$ be the probability of a fragment being bound by a protein or histone modification and $P_{kept}$ be a modifier that affects the probability that a fragment will be kept. 
$$P_b = \text{peak score} * P_{kept} \text{ where } P_{kept} = \dfrac{numreads * rate_{PCR}}{numfrags_{run} * numcopies}$$
$P_{kept}$ is reliant on four variables, $numreads$, $rate_{PCR}$, $numfrags_{run}$, and $numcopies$. $numreads$ and $numcopies$ are parameters inputted by the user that indicate the approximate total amount of reads that should be outputted to the fastq files and the total number of cells in our experiment respectively. The total amount of cells indicates how many genomes we are sequencing and has a heavy influence on the coverage of each peak. $rate_{PCR}$ is also a user defined parameter, however it can be generated from our learn function by analyzing the duplicated fragments in the inputted bam file. Lastly, $numfrags_{run}$ is representative of the total amount of fragments we expect to be pulled down per run, where a run is iterating over the entire region of the genome specified one time. Thus, when you have the quantity: $numfrags_{run}*numcopies$ it represents the total amount of fragments we expect throughout the pulldown process. The quantity $numreads*rate_{PCR}$ represents the total amount of unique reads that will be generated from the pulldown process. Thus $P_{kept}$ is the ratio of the total unique reads to total amount of fragments expected which is a necessity to ensure the proper amount of reads will be generated and outputted. 

Now that we have the probability of a fragment being bound, there are two outcomes. The fragment is bound and will be used for PCR and sequencing or the fragment is not bound and we now need to determine whether it should be pulled down as noise or filtered out. The probability defined above is the initial test as to whether the fragment should be pulled down. If the fragment is ultimately not bound, we now need to find the net probability that noise will be pulled down which we define as $P_{noise}$. This quantity is determined as:
$$P_{noise} = P_{kept}*P_{avg}(Pd|UB)$$
where $P_{avg}(Pd|UB)$ is defined as the probability the fragment is pulled down given that it is unbound and $P_{kept}$ is the same as listed above. Using Bayes Theorem, we can define
$$P_{avg}(Pd|UB) = \dfrac{P(UB|Pd)*P(Pd)}{P(UB)}$$
$$P_{avg}(Pd|B) = \dfrac{P(B|Pd)*P(Pd)}{P(B)}$$
and we can also find:
$$Prob_{avg}(Pd|B) = average(P_x(Pd|B))$$
where $P_x(Pd|B) = \dfrac{coverage(y)}{numcopies}$ is defined as the probability for a single given fragment $x$, which overlaps the peak $y$, that it is pulled down given that it is bound. $coverage(y)$ is defined as the total number of reads that span this peak and $numcopies$ is the same parameter as described above. However, we can not find $P(Pd)$, so we need to remove this probability in order to find $P_{avg}(Pd|UB)$.
Thus if we take the ratio:
$$\dfrac{P_{avg}(Pd|B)}{P_{avg}(Pd|UB)} = \dfrac{P(B|Pd)*P(UB)}{P(B)*P(UB|Pd)}$$

we can solve for $P_{avg}(Pd|UB)$ with having the term $P(Pd)$:
$$P_{avg}(Pd|UB) = \dfrac{average(P_x(Pd|B))*P(B)*P(UB|Pd)}{P(B|Pd)*P(UB)}$$
Breaking this into components we can solve for $\dfrac{P(B|Pd)}{P(UB|Pd)}$ and $\dfrac{P(UB)}{P(B)}$
$\dfrac{P(B|Pd)}{P(UB|Pd)} = \dfrac{tagcount(B|Pd)}{tagcount(UP|Pd)}$
where: 
$$tagcount(B|Pd) = tagcount(B, called|Pd) + tagcount(B, not called|Pd)$$
$$tagcount(called|Pd) = tagcount(B, called|Pd) + tagcount(UB, called|Pd)$$
All of these values can be determined through mapping reads from the BAM file to the ChIP-seq peaks in the bed file and calling all mapped to peak locations as bound and all others as unbound. However, to determine the values the amount of the bound fragments that were not used in the BAM we can estimate this as it should be proportional to the amount of noise in the file:
$$tagcount(B, called|P) + tagcount(B, not called|P) \approx tagcount(B, called|P) + tagcount(UB, called|P)$$

From ChIP-seq peaks, we can acquire: $length(B)$ and $length(UB)$ which by taking the ratio of the length of fragments bound we can determine the ratio of probabilities of unbound to bound.

$$\dfrac{P(UB)}{P(B)} = \dfrac{length(UB)}{length(B)}$$

In order to gather these lengths we need to look at all the fragments in the file and the lengths that they span when under and peak and not under a peak. $length(called)$ is defined as the total length of fragments that spans the genome. 
$$length(B) = length(B, called) + length(B, not called)$$
$$length(called) = length(B, called) + length(UB, called)$$
and assume that:
$$length(B, called) + length(B, not called) \approx length(B, called) + length(UB, called)$$

Finally, we can solve for $P_{avg}(Pd|UB)$:
$$P_{avg}(Pd|UB) = \dfrac{average(P_x(Pd|B))*length(B)*tagcount(UB|Pd)}{length(UB)*tagcount(B|Pd)}$$

This ultimately leads us back to $P_{noise} = P_{kept}*P_{avg}(Pd|UB)$ which we can now calculate in order to understand whether a not this unbound fragment should be pulled down.

\subsubsection*{PCR}

PCR (Polymerase Chain Reaction) is the process of replicating currently existing sequence fragments that have been pulled down. In our model this is determined through a user defined parameter, $PCR_{rate}$, that can be generated from our learn module. We assumed the number of copies followed a geometric distribution with its mean value as $\mu$, and estimated $\mu$ using equation (1) in which $n_i$ represents the number of reads with i PCR copies (including the original one). The $PCR_{rate}$ was calculated with equation (2).
$$\mu = \frac{\sum_{i=1}^\infty (i * n_i)}{\sum_{i=1}^\infty n_i} \;\;\;(1)$$
$$PCR_{rate} = \frac{1}{\mu}\;\;\;(2)$$

\subsubsection*{Sequencing}

After all the fragments have been pulled down, it is time to generate the fastq reads through the sequencing process. At the start of this process we go through each fragment that was successfully pulled down and use its chromosome, start, and end positions to grab the sequence from the reference genome. If the sequencing method is paired end we take the first $n$ nucleotides, where $n$ is decided by the user input parameter readlen, and write out this read to the first fastq file. Then we take the reverse complement of the last $n$ nucleotides and output this to the second fastq file matching the index of the first. Otherwise if single end was specified, only the first $n$ nucleotides are outputted to a single a fastq.


\subsection*{ChIPmunk implementation}

two modules
file formats
binning
C++, any special libraries we use, open source

\subsection*{Peak caller evaluation}



\subsection*{Data sources}



\end{document}

