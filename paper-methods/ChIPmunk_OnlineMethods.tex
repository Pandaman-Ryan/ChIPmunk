\documentclass[12pt]{article}

% Imports
\usepackage{hyperref}
\usepackage[margin=0.5in]{geometry}
\usepackage{ctable}
\usepackage{array}
\usepackage{titlesec}
\usepackage{amsmath}

% Paragraph spacing
\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}

% Default font
\renewcommand*{\familydefault}{\sfdefault}

% title spacing
\titlespacing*{\section}
{0pt}{2pt}{0pt}
\titlespacing*{\subsection}
{0pt}{2pt}{0pt}

% table lines
\newcolumntype{?}{!{\vrule width 1pt}}

% hyperlinks
\hypersetup{
  breaklinks=true,  % so long urls are correctly broken across lines
  colorlinks=true,
  urlcolor=blue,
  linkcolor=red,
  citecolor=red,
 }

\begin{document}

%%% Notes %%%%
% * Put one sentence per line to make github track changes easier
% * Do not hard code references to figures, equations in this doc. use \ref{label} instead
\section*{Online Methods}

\subsection*{ChIPmunk model}

ChIPmunk models each major step (shearing, pulldown, PCR, and sequencing) of the ChIP-seq protocol (\textbf{Figure 1A, steps 1-4}) as a distinct module. It assumes binding sites for the target epitome are known, unlike other tools [refs] which focus on simulating binding sites themselves. Model parameters are summarized in \textbf{Table 1}.

\subsubsection*{1. Shearing}

In step 1, cross-linked DNA is sheared to a target fragment length, typically by sonication.
ChIPmunk models the length distribution of fragments.
We model fragment lengths using a gamma distribution (\textbf{Figure 1B}) based on empirical observation of fragment distributions which have long right tails (\textbf{Supplementary Figure 2}).
In the case of paired-end reads, fragment lengths can be determined trivially from the mapping locations of paired reads.
For single-end reads, individual fragment lengths are not directly observed. We outline a novel method for inferring summary statistics for the length distribution using single-end reads.

\paragraph{Inferring fragment lengths from paired-end reads}
The observed fragment length ($X_i$) for each read pair $i$ can be computed based on the mapping coordinates of the two reads.
The learn module randomly selects 10,000 read pairs from the input BAM for fitting a gamma distribution.
Read pairs are filtered to remove fragments marked as duplicates or secondary alignments. % TODO what about if not proper pair? and not mapping to same chrom?
Read pairs are further filtered to remove fragments with length greater than 3 times the median length of selected fragments.

The mean fragment length is easily computed as $\mu = \dfrac{\sum_{i=1}^{n}X_i}{n}$, where $n$ is the number of fragments remaining after filtering. We then use the method of moments to find maximum likelihood estimates of the gamma distribution shape ($k$) and scale parameters ($\theta$):

\begin{equation}
  k =  \dfrac{\mu}{\theta}
\end{equation}
\begin{equation}
  \theta = \dfrac{1}{n\mu}\sum_{i=1}^{n}(X_i - \mu)^2
\end{equation}

\paragraph{Inferring fragment lengths from single-end reads}
To estimate the fragment length distribution from single-end reads, we assume the length distribution follows gamma distribution with the mean $\mu$ and variance $v$, and use reads located inside ChIP-seq peaks (provided as input) to estimate $\mu$ and $v$ which are used to compute $k$ and $\theta$.

For each peak $peak_i$, we keep track of two lists, $\{start\}_{peak_i}$ and $\{end\}_{peak_i}$.
For each read overlapping $peak_i$, if the read is on the forward strand we add its start coordinate $\{start\}_{peak_i}$.
If the read is on the reverse strand we add its start coordinate to $\{end\}_{peak_i}$.
The center point of this peak is calculated as:

\begin{equation} \label{eq:center}
  center_{peak_i} = \frac{mean(\{start\}_{peak_i}) + mean(\{end\}_{peak_i})}{2}
\end{equation}
  
For every $peak_i$ we offset the coordinates in $\{start\}_{peak_i}$ and $\{end\}_{peak_i}$ by $center_{peak_i}$t, so that the coordinates of start points and end points are normalized and symmetric around zero.
We then concatenate lists from each peak to form $\{start\}$ and $\{end\}$:

\begin{equation} \label{eq:concat}
  \begin{array}{c}
  \{start\} = \oplus_{i=0}^{n} (\{start\}_{peak_i} - center_{peak_i})\\
  \{end\} = \oplus_{i=0}^{n} (\{end\}_{peak_i} - center_{peak_i})
  \end{array}
\end{equation}

The mean value of fragment length mu can be estimated as:
\begin{equation}
  \mu = mean(\{end\}) - mean(\{start\})
\end{equation}

We calculate the probability density functions, cumulative density functions and expected density functions for both $\{start\}$ and $\{end\}$.
The expected density function $EDF(x)$ is defined as the expected deviation of a random element in the list to $x$:
\begin{equation} \label{eq:EDF}
  \begin{array}{c} 
    EDF_{start}(x) = E(|S - x|) \\
    EDF_{end}(x) = E(|E - x|)
    \end{array}
\end{equation}
where $S$ is a random element in $\{start\}$ and $E$ is a random element in $\{end\}$.

Since we can compute $\mu$, we can reduce the density function of the fragment length distribution to $p_v(x)$. % TODO we don't really define this? how relates to F(v)?
We construct a score function $F(v)$ as shown below.
Intuitively, if we have a correct guess of $v$, $F(v)$ should be equal to zero.

\begin{equation}
  \begin{array}{c} \label{eq:Fv}
F(v) = E_v(|S + \frac{L}{2}|) + E_v(|E - \frac{L}{2}|) - E(|S + \frac{\mu}{2}|) + E(|E- \frac{\mu}{2}|) \\
E_v(|S + L/2|) = \sum_{x=0}^\infty p_v(x) * EDF_{start}(-\frac{x}{2}) \\
E_v(|E - L/2|) = \sum_{x=0}^\infty p_v(x) * EDF_{end}(\frac{x}{2}) \\
E(|S + \frac{\mu}{2}|)=EDF_{start}(x) \\
E(|E - \frac{\mu}{2}|)=EDF_{end}(x) \\
\end{array}
\end{equation}

To find an optimal $v$ that minimizes $|F(v)|$, we conduct a binary search between 1000 and 10,000.

In practice, we slightly offset the last two items in the score function in \textbf{Equation~\ref{eq:Fv}} to get the score below, which gave slightly more accurate estimation of $v$ on real data. This may be due to the fact that fragment length distributions are truncated on the left end, with little or no fragments with lengths less than 100bp observed, and thus do not follow a true gamma distribution.

\begin{equation}
F(v) = E_v(|S + \frac{L}{2}|) + E_v(|E - \frac{L}{2}|) - E(|S + \frac{\mu}{2} - \frac{E- \frac{\mu}{2}}{4}|) - E(|E- \frac{\mu}{2} - \frac{S + \frac{\mu}{2}}{4}|)
\end{equation}

\paragraph{Generating fragments}
% TODO generate fragments from gamma distribution by chopping up chromosomes
% TODO mention num copies issue here

% Using these new found parameters we can generate fragments of appropriate sizes to be used in the pulldown phase of this simulator.

\subsubsection*{Pulldown}

The pulldown step of the simulator represents purifying the precipitated DNA by washing away the sheared fragments that were not bound by the target protein or histone modification. These filtered fragments are then marked to be sequenced. The filtering process is not perfect and therefore some fragments not bound will also be pulled down to be sequenced which we label as noise. In order to model this process we start at a given position in the genome based on the region specified by the user. The random fragment is generated from the gamma distribution learned through the shearing process and based on the fragment's location we can generate its peak score. The peak score is a metric based on the peak intensity read from the bed file that the user has inputted. The higher the intensity of the peak the higher the probability that a fragment will be mapped to that location. However, this isn't the only factor to determine if the fragment will be bound. Let $P_b$ be the probability of a fragment being bound by a protein or histone modification and $P_{kept}$ be a modifier that affects the probability that a fragment will be kept. 
$$P_b = \text{peak score} * P_{kept} \text{ where } P_{kept} = \dfrac{numreads * rate_{PCR}}{numfrags_{run} * numcopies}$$
$P_{kept}$ is reliant on four variables, $numreads$, $rate_{PCR}$, $numfrags_{run}$, and $numcopies$. $numreads$ and $numcopies$ are parameters inputted by the user that indicate the approximate total amount of reads that should be outputted to the fastq files and the total number of cells in our experiment respectively. The total amount of cells indicates how many genomes we are sequencing and has a heavy influence on the coverage of each peak. $rate_{PCR}$ is also a user defined parameter, however it can be generated from our learn function by analyzing the duplicated fragments in the inputted bam file. Lastly, $numfrags_{run}$ is representative of the total amount of fragments we expect to be pulled down per run, where a run is iterating over the entire region of the genome specified one time. Thus, when you have the quantity: $numfrags_{run}*numcopies$ it represents the total amount of fragments we expect throughout the pulldown process. The quantity $numreads*rate_{PCR}$ represents the total amount of unique reads that will be generated from the pulldown process. Thus $P_{kept}$ is the ratio of the total unique reads to total amount of fragments expected which is a necessity to ensure the proper amount of reads will be generated and outputted. 

Now that we have the probability of a fragment being bound, there are two outcomes. The fragment is bound and will be used for PCR and sequencing or the fragment is not bound and we now need to determine whether it should be pulled down as noise or filtered out. The probability defined above is the initial test as to whether the fragment should be pulled down. If the fragment is ultimately not bound, we now need to find the net probability that noise will be pulled down which we define as $P_{noise}$. This quantity is determined as:
$$P_{noise} = P_{kept}*P_{avg}(Pd|UB)$$
where $P_{avg}(Pd|UB)$ is defined as the probability the fragment is pulled down given that it is unbound and $P_{kept}$ is the same as listed above. Using Bayes Theorem, we can define
$$P_{avg}(Pd|UB) = \dfrac{P(UB|Pd)*P(Pd)}{P(UB)}$$
$$P_{avg}(Pd|B) = \dfrac{P(B|Pd)*P(Pd)}{P(B)}$$
and we can also find:
$$Prob_{avg}(Pd|B) = average(P_x(Pd|B))$$
where $P_x(Pd|B) = \dfrac{coverage(y)}{numcopies}$ is defined as the probability for a single given fragment $x$, which overlaps the peak $y$, that it is pulled down given that it is bound. $coverage(y)$ is defined as the total number of reads that span this peak and $numcopies$ is the same parameter as described above. However, we can not find $P(Pd)$, so we need to remove this probability in order to find $P_{avg}(Pd|UB)$.
Thus if we take the ratio:
$$\dfrac{P_{avg}(Pd|B)}{P_{avg}(Pd|UB)} = \dfrac{P(B|Pd)*P(UB)}{P(B)*P(UB|Pd)}$$

we can solve for $P_{avg}(Pd|UB)$ with having the term $P(Pd)$:
$$P_{avg}(Pd|UB) = \dfrac{average(P_x(Pd|B))*P(B)*P(UB|Pd)}{P(B|Pd)*P(UB)}$$
Breaking this into components we can solve for $\dfrac{P(B|Pd)}{P(UB|Pd)}$ and $\dfrac{P(UB)}{P(B)}$
$\dfrac{P(B|Pd)}{P(UB|Pd)} = \dfrac{tagcount(B|Pd)}{tagcount(UP|Pd)}$
where: 
$$tagcount(B|Pd) = tagcount(B, called|Pd) + tagcount(B, not called|Pd)$$
$$tagcount(called|Pd) = tagcount(B, called|Pd) + tagcount(UB, called|Pd)$$
All of these values can be determined through mapping reads from the BAM file to the ChIP-seq peaks in the bed file and calling all mapped to peak locations as bound and all others as unbound. However, to determine the values the amount of the bound fragments that were not used in the BAM we can estimate this as it should be proportional to the amount of noise in the file:
$$tagcount(B, called|P) + tagcount(B, not called|P) \approx tagcount(B, called|P) + tagcount(UB, called|P)$$

From ChIP-seq peaks, we can acquire: $length(B)$ and $length(UB)$ which by taking the ratio of the length of fragments bound we can determine the ratio of probabilities of unbound to bound.

$$\dfrac{P(UB)}{P(B)} = \dfrac{length(UB)}{length(B)}$$

In order to gather these lengths we need to look at all the fragments in the file and the lengths that they span when under and peak and not under a peak. $length(called)$ is defined as the total length of fragments that spans the genome. 
$$length(B) = length(B, called) + length(B, not called)$$
$$length(called) = length(B, called) + length(UB, called)$$
and assume that:
$$length(B, called) + length(B, not called) \approx length(B, called) + length(UB, called)$$

Finally, we can solve for $P_{avg}(Pd|UB)$:
$$P_{avg}(Pd|UB) = \dfrac{average(P_x(Pd|B))*length(B)*tagcount(UB|Pd)}{length(UB)*tagcount(B|Pd)}$$

This ultimately leads us back to $P_{noise} = P_{kept}*P_{avg}(Pd|UB)$ which we can now calculate in order to understand whether a not this unbound fragment should be pulled down.

\subsubsection*{PCR}

PCR (Polymerase Chain Reaction) is the process of replicating currently existing sequence fragments that have been pulled down. In our model this is determined through a user defined parameter, $PCR_{rate}$, that can be generated from our learn module. We assumed the number of copies followed a geometric distribution with its mean value as $\mu$, and estimated $\mu$ using equation (1) in which $n_i$ represents the number of reads with i PCR copies (including the original one). The $PCR_{rate}$ was calculated with equation (2).
$$\mu = \frac{\sum_{i=1}^\infty (i * n_i)}{\sum_{i=1}^\infty n_i} \;\;\;(1)$$
$$PCR_{rate} = \frac{1}{\mu}\;\;\;(2)$$

\subsubsection*{Sequencing}

After all the fragments have been pulled down, it is time to generate the fastq reads through the sequencing process. At the start of this process we go through each fragment that was successfully pulled down and use its chromosome, start, and end positions to grab the sequence from the reference genome. If the sequencing method is paired end we take the first $n$ nucleotides, where $n$ is decided by the user input parameter readlen, and write out this read to the first fastq file. Then we take the reverse complement of the last $n$ nucleotides and output this to the second fastq file matching the index of the first. Otherwise if single end was specified, only the first $n$ nucleotides are outputted to a single a fastq.


\subsection*{ChIPmunk implementation}

two modules
file formats
binning
C++, any special libraries we use, open source

\subsection*{Peak caller evaluation}



\subsection*{Data sources}



\end{document}

